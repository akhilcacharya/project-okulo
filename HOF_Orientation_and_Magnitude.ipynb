{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import sqrt, pi, arctan2, cos, sin, stats\n",
    "from scipy.ndimage import uniform_filter\n",
    "\n",
    "original_frame1 = cv2.imread(\"data/tennis492.jpg\")\n",
    "original_frame2 = cv2.imread(\"data/tennis493.jpg\")\n",
    "\n",
    "# convert to gray-scale\n",
    "frame1 = cv2.cvtColor(original_frame1,cv2.COLOR_BGR2GRAY)\n",
    "frame2 = cv2.cvtColor(original_frame2,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "frame1 = cv2.resize(frame1, (256,256))\n",
    "frame2 = cv2.resize(frame2, (256,256))\n",
    "\n",
    "# Gets the optical flow [<dx,dy>] from two frames\n",
    "def getOpticalFlow(imPrev, imNew):\n",
    "    flow = cv2.calcOpticalFlowFarneback(imPrev, imNew, flow=None, pyr_scale=.5, levels=3, winsize=9, iterations=1, poly_n=3, poly_sigma=1.1, flags=cv2.OPTFLOW_FARNEBACK_GAUSSIAN)\n",
    "    return flow\n",
    "\n",
    "flow = getOpticalFlow(frame1, frame2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the Histogram of Optical Flow (HoF) from the given optica flow\n",
    "\n",
    "def hof(flow, orientations=9, pixels_per_cell=(10, 10),\n",
    "        cells_per_block=(3, 3), visualise=False, normalise=False, motion_threshold=1.):\n",
    "    flow = np.atleast_2d(flow)\n",
    "    if flow.ndim < 3:\n",
    "        raise ValueError(\"Requires dense flow in both directions\")\n",
    "\n",
    "    if normalise:\n",
    "        flow = sqrt(flow)\n",
    "\n",
    "    if flow.dtype.kind == 'u':\n",
    "        # convert uint image to float\n",
    "        # to avoid problems with subtracting unsigned numbers in np.diff()\n",
    "        flow = flow.astype('float')\n",
    "\n",
    "    gx = np.zeros(flow.shape[:2])\n",
    "    gy = np.zeros(flow.shape[:2])\n",
    "    # gx[:, :-1] = np.diff(flow[:,:,1], n=1, axis=1)\n",
    "    # gy[:-1, :] = np.diff(flow[:,:,0], n=1, axis=0)\n",
    "\n",
    "    gx = flow[:,:,1]\n",
    "    gy = flow[:,:,0]\n",
    "    \n",
    "    magnitude = sqrt(gx**2 + gy**2)\n",
    "    orientation = arctan2(gy, gx) * (180 / pi) % 180\n",
    "\n",
    "    sy, sx = flow.shape[:2]\n",
    "    cx, cy = pixels_per_cell\n",
    "    bx, by = cells_per_block\n",
    "\n",
    "    n_cellsx = int(np.floor(sx // cx))  # number of cells in x\n",
    "    n_cellsy = int(np.floor(sy // cy))  # number of cells in y\n",
    "\n",
    "    # compute orientations integral images\n",
    "    orientation_histogram = np.zeros((n_cellsy, n_cellsx, orientations))\n",
    "    subsample = np.index_exp[cy / 2:cy * n_cellsy:cy, cx / 2:cx * n_cellsx:cx]\n",
    "    for i in range(orientations-1):\n",
    "        #create new integral image for this orientation\n",
    "        # isolate orientations in this range\n",
    "\n",
    "        temp_ori = np.where(orientation < 180 / orientations * (i + 1),\n",
    "                            orientation, -1)\n",
    "        temp_ori = np.where(orientation >= 180 / orientations * i,\n",
    "                            temp_ori, -1)\n",
    "        # select magnitudes for those orientations\n",
    "        cond2 = (temp_ori > -1) * (magnitude > motion_threshold)\n",
    "        temp_mag = np.where(cond2, magnitude, 0)\n",
    "\n",
    "        temp_filt = uniform_filter(temp_mag, size=(cy, cx))\n",
    "        orientation_histogram[:, :, i] = temp_filt[subsample]\n",
    "\n",
    "    ''' Calculate the no-motion bin '''\n",
    "    temp_mag = np.where(magnitude <= motion_threshold, magnitude, 0)\n",
    "\n",
    "    temp_filt = uniform_filter(temp_mag, size=(cy, cx))\n",
    "    orientation_histogram[:, :, -1] = temp_filt[subsample]\n",
    "\n",
    "    # now for each cell, compute the histogram\n",
    "    hof_image = None\n",
    "\n",
    "    if visualise:\n",
    "        from skimage import draw\n",
    "\n",
    "        radius = min(cx, cy) // 2 - 1\n",
    "        hof_image = np.zeros((sy, sx), dtype=float)\n",
    "        for x in range(n_cellsx):\n",
    "            for y in range(n_cellsy):\n",
    "                for o in range(orientations-1):\n",
    "                    centre = tuple([y * cy + cy // 2, x * cx + cx // 2])\n",
    "                    dx = int(radius * cos(float(o) / orientations * np.pi))\n",
    "                    dy = int(radius * sin(float(o) / orientations * np.pi))\n",
    "                    rr, cc = draw.bresenham(centre[0] - dy, centre[1] - dx,\n",
    "                                            centre[0] + dy, centre[1] + dx)\n",
    "                    hof_image[rr, cc] += orientation_histogram[y, x, o]\n",
    "\n",
    "    \"\"\"\n",
    "    The fourth stage computes normalisation, which takes local groups of\n",
    "    cells and contrast normalises their overall responses before passing\n",
    "    to next stage. Normalisation introduces better invariance to illumination,\n",
    "    shadowing, and edge contrast. It is performed by accumulating a measure\n",
    "    of local histogram \"energy\" over local groups of cells that we call\n",
    "    \"blocks\". The result is used to normalise each cell in the block.\n",
    "    Typically each individual cell is shared between several blocks, but\n",
    "    its normalisations are block dependent and thus different. The cell\n",
    "    thus appears several times in the final output vector with different\n",
    "    normalisations. This may seem redundant but it improves the performance.\n",
    "    We refer to the normalised block descriptors as Histogram of Oriented\n",
    "    Gradient (hog) descriptors.\n",
    "    \"\"\"\n",
    "\n",
    "    n_blocksx = (n_cellsx - bx) + 1\n",
    "    n_blocksy = (n_cellsy - by) + 1\n",
    "    normalised_blocks = np.zeros((n_blocksy, n_blocksx,\n",
    "                                  by, bx, orientations))\n",
    "\n",
    "    for x in range(n_blocksx):\n",
    "        for y in range(n_blocksy):\n",
    "            block = orientation_histogram[y:y+by, x:x+bx, :]\n",
    "            eps = 1e-5\n",
    "            normalised_blocks[y, x, :] = block / sqrt(block.sum()**2 + eps)\n",
    "\n",
    "    \"\"\"\n",
    "    The final step collects the hof descriptors from all blocks of a dense\n",
    "    overlapping grid of blocks covering the detection window into a combined\n",
    "    feature vector for use in the window classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    if visualise:\n",
    "        return normalised_blocks.ravel(), hof_image\n",
    "    else:\n",
    "        return normalised_blocks.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,  -2.66055072e-19\n",
      "  -1.96920303e-34   1.25529637e-03]\n",
      "(42849,)\n"
     ]
    }
   ],
   "source": [
    "hof_result = hof(flow)\n",
    "print hof_result\n",
    "print hof_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic\n",
    "def hofom(flow, orientations=4, magnitudes=4, pixels_per_cell=(8,8)):\n",
    "    gx = flow[:,:,1]\n",
    "    gy = flow[:,:,0]\n",
    "    \n",
    "    magnitude = sqrt(gx**2 + gy**2)\n",
    "    magnitude_ravel = magnitude.ravel()\n",
    "    magnitude_ranked = stats.rankdata(magnitude_ravel, \"average\")/float(len(magnitude_ravel))\n",
    "    orientation = arctan2(gy, gx) * (180 / pi) % 180\n",
    "    orientation_ravel = orientation.ravel()\n",
    "    orientation_ranked = stats.rankdata(orientation_ravel, \"average\")/float(len(orientation_ravel))\n",
    "    \n",
    "    sy, sx = flow.shape[:2]\n",
    "    cx, cy = pixels_per_cell\n",
    "    ncx = sx//cx\n",
    "    ncy = sy//cy\n",
    "    \n",
    "    ret = np.zeros((ncx, ncy, orientations*magnitudes))\n",
    "    \n",
    "    #(256,256) -> (32,32,16)\n",
    "    for i in range(sy):\n",
    "        for j in range(sx):\n",
    "            # map ori to {orientations} values\n",
    "            pori = orientation_ranked[i*sx+j]\n",
    "            iori = int(pori * orientations + 1)\n",
    "            # map mag to {magnitudes} values\n",
    "            pmag = magnitude_ranked[i*sx+j]\n",
    "            imag = int(pmag * magnitudes + 1)\n",
    "            iy = i%ncy\n",
    "            ix = j%ncx\n",
    "            quantile = iori*imag-1\n",
    "            if quantile > orientations*magnitudes:\n",
    "                quantile = orientations*magnitudes - 1\n",
    "            ret[ix, iy, quantile] += 1\n",
    "    return ret.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25,  1.  ,  0.5 ,  0.75])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mag_ranked = stats.rankdata([1,23,4,5], \"min\")/4.\n",
    "mag_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = hofom(flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16384,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nc: 23\n",
      "offset: 20\n",
      "train files: 40\n",
      "train labels: 40\n",
      "test files: 6\n",
      "test labels: 6\n"
     ]
    }
   ],
   "source": [
    "# Adding a modifier to reduce number of dataset\n",
    "MODIFIER=1\n",
    "\n",
    "# Collect the file path of all the running and walking videos, we will only be using these 2 classes\n",
    "RUN_DIR = \"hof/run/\"\n",
    "RUN_FILES = os.listdir(RUN_DIR)\n",
    "RUN_FILES = [RUN_DIR + f for f in RUN_FILES]\n",
    "WALK_DIR = \"hof/walk/\"\n",
    "WALK_FILES = os.listdir(WALK_DIR)\n",
    "WALK_FILES = [WALK_DIR + f for f in WALK_FILES]\n",
    "\n",
    "# Use equal number of data from each class\n",
    "nc = min(len(RUN_FILES), len(WALK_FILES)) / MODIFIER\n",
    "print \"nc:\", nc\n",
    "RUN_FILES = RUN_FILES[0:nc]\n",
    "WALK_FILES = WALK_FILES[0:nc]\n",
    "\n",
    "RATIO = 0.9\n",
    "offset = int(np.floor(nc*RATIO))\n",
    "print \"offset:\", offset\n",
    "\n",
    "# Split test and training at a ratio of 1:9\n",
    "train_files = RUN_FILES[0:offset] + WALK_FILES[0:offset]\n",
    "test_files = RUN_FILES[offset:nc] + WALK_FILES[offset:nc]\n",
    "\n",
    "# Put the labels in vectors\n",
    "train_labels = np.zeros(offset*2, int)\n",
    "train_labels[0:offset] = 1 #RUN=1\n",
    "train_labels[offset:offset*2] = 2 #WALK=2\n",
    "\n",
    "test_len = nc-offset\n",
    "test_labels = np.zeros(test_len*2, int)\n",
    "test_labels[0:test_len] = 1 #RUN=1\n",
    "test_labels[test_len:test_len*2] = 2 #WALK=2\n",
    "\n",
    "print \"train files:\", len(train_files)\n",
    "print \"train labels:\", len(train_labels)\n",
    "print \"test files:\", len(test_files)\n",
    "print \"test labels:\", len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getOpticalFlow(imPrev, imNew):\n",
    "    flow = cv2.calcOpticalFlowFarneback(imPrev, imNew, flow=None, pyr_scale=.5, levels=3, winsize=9, iterations=1, poly_n=3, poly_sigma=1.1, flags=cv2.OPTFLOW_FARNEBACK_GAUSSIAN)\n",
    "    return flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FIXED_WIDTH = 256\n",
    "FIXED_HEIGHT = 256\n",
    "def normalizeFrame(frame_original):\n",
    "    frame_gray = cv2.cvtColor(frame_original,cv2.COLOR_BGR2GRAY)\n",
    "    frame_gray_resized = cv2.resize(frame_gray, (FIXED_WIDTH, FIXED_HEIGHT))\n",
    "    return frame_gray_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the Histogram of Optical Flow from two images\n",
    "def getHOFOM(frame1, frame2):\n",
    "    flow = getOpticalFlow(frame1, frame2)\n",
    "    return hofom(flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the Histogram of Optical Flows of a video grouped sequentially in a 1D array\n",
    "def getSequentialHOFOM(video_path):\n",
    "    print \"Processing\", video_path\n",
    "    hofs = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret1, frame1 = cap.read()\n",
    "    frame1 = normalizeFrame(frame1)\n",
    "    while(cap.isOpened()):\n",
    "        ret2, frame2 = cap.read()\n",
    "        if ret2 == True:\n",
    "            frame2 = normalizeFrame(frame2)\n",
    "            hof_array = getHOFOM(frame1, frame2)\n",
    "            hofs = np.concatenate((hofs, hof_array),axis=0)\n",
    "#             show2images(frame1, frame2)\n",
    "            frame1 = frame2\n",
    "        else:\n",
    "            break\n",
    "    return hofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hof/run/20060723sfjffangelina_run_f_cm_np2_fr_med_3.avi\n",
      "Processing hof/run/20060723sfjffangelina_run_f_nm_np1_ri_med_1.avi\n",
      "Processing hof/run/20060723sfjffangelina_run_f_nm_np1_ri_med_2.avi\n",
      "Processing hof/run/20060723sfjffbartsinger_run_f_nm_np1_fr_med_1.avi\n",
      "Processing hof/run/20060723sfjffbumblebeesuitman_run_f_cm_np2_ri_med_1.avi\n",
      "Processing hof/run/20060723sfjffinallseriousness_run_f_cm_np1_fr_med_0.avi\n",
      "Processing hof/run/20060723sfjffinallseriousness_run_f_cm_np2_ba_med_2.avi\n",
      "Processing hof/run/20060723sfjffinallseriousness_run_u_cm_np1_fr_med_1.avi\n",
      "Processing hof/run/20060723sfjffjewcy_run_f_nm_np1_ba_med_3.avi\n",
      "Processing hof/run/20060723sfjffsomelikeitwarmed_run_f_nm_np1_fr_med_6.avi\n",
      "Processing hof/run/50_FIRST_DATES_run_f_cm_np1_ba_med_12.avi\n",
      "Processing hof/run/50_FIRST_DATES_run_f_cm_np1_ri_med_21.avi\n",
      "Processing hof/run/50_FIRST_DATES_run_f_nm_np1_ba_med_20.avi\n",
      "Processing hof/run/50_FIRST_DATES_run_f_nm_np1_fr_med_34.avi\n",
      "Processing hof/run/A_Beautiful_Mind_4_run_f_cm_np1_ba_med_9.avi\n",
      "Processing hof/run/A_Beautiful_Mind_4_run_u_cm_np2_fr_med_10.avi\n",
      "Processing hof/run/A_Beautiful_Mind_5_run_f_nm_np1_fr_bad_6.avi\n",
      "Processing hof/run/A_Beautiful_Mind_5_run_u_cm_np1_fr_med_7.avi\n",
      "Processing hof/run/Age13_1_run_f_nm_np1_fr_med_1.avi\n",
      "Processing hof/run/Age13_1_run_f_nm_np1_fr_med_2.avi\n",
      "Processing hof/walk/20060723sfjffbitemebaby_walk_u_nm_np1_fr_med_0.avi\n",
      "Processing hof/walk/20060723sfjffbumblebeesuitman_walk_u_cm_np2_fr_med_0.avi\n",
      "Processing hof/walk/20060723sfjffcomeback_walk_u_cm_np1_ri_med_2.avi\n",
      "Processing hof/walk/20060723sfjffcookies_walk_u_nm_np1_ba_med_1.avi\n",
      "Processing hof/walk/20060723sfjffcookies_walk_u_nm_np1_fr_med_0.avi\n",
      "Processing hof/walk/20060723sfjffjewcy_walk_f_cm_np1_ba_med_2.avi\n",
      "Processing hof/walk/20060723sfjffjewcy_walk_f_nm_np1_fr_med_0.avi\n",
      "Processing hof/walk/20060723sfjffjewcy_walk_f_nm_np1_fr_med_1.avi\n",
      "Processing hof/walk/20060723sfjffjewgotmail_walk_f_cm_np2_ba_med_2.avi\n",
      "Processing hof/walk/20060723sfjffjewgotmail_walk_f_nm_np1_fr_med_1.avi\n",
      "Processing hof/walk/20060723sfjffkillerskiss_walk_f_cm_np1_fr_med_1.avi\n",
      "Processing hof/walk/20060723sfjffmyspot_walk_f_nm_np1_ri_med_0.avi\n",
      "Processing hof/walk/20060723sfjffprofessionalhelp_walk_u_nm_np2_le_med_0.avi\n",
      "Processing hof/walk/20060723sfjffschmoozer_walk_f_cm_np1_ri_med_1.avi\n",
      "Processing hof/walk/20060723sfjffschmoozer_walk_f_cm_np2_ri_med_0.avi\n",
      "Processing hof/walk/20060723sfjffsomelikeitwarmed_walk_f_cm_np1_ba_med_0.avi\n",
      "Processing hof/walk/20060723sfjffsomelikeitwarmed_walk_f_nm_np1_ba_med_4.avi\n",
      "Processing hof/walk/20070723_sfjff_waitingforrescue_walk_f_nm_np1_ba_med_1.avi\n",
      "Processing hof/walk/21_walk_h_cm_np1_fr_med_10.avi\n",
      "Processing hof/walk/21_walk_u_cm_np1_fr_bad_3.avi\n",
      "Processing hof/run/Age13_2_run_f_cm_np1_ba_med_1.avi\n",
      "Processing hof/run/AgentCodyBanks_run_f_cm_np1_ba_med_11.avi\n",
      "Processing hof/run/AgentCodyBanks_run_u_cm_np1_le_med_13.avi\n",
      "Processing hof/walk/21_walk_u_cm_np1_fr_med_0.avi\n",
      "Processing hof/walk/21_walk_u_cm_np1_fr_med_11.avi\n",
      "Processing hof/walk/21_walk_u_cm_np1_fr_med_15.avi\n"
     ]
    }
   ],
   "source": [
    "train = [getSequentialHOFOM(p) for p in train_files]\n",
    "test = [getSequentialHOFOM(p) for p in test_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1703936\n",
      "1261568\n"
     ]
    }
   ],
   "source": [
    "print len(test[1])\n",
    "print len(test[2])\n",
    "max_width = max(np.array([len(i) for i in train]).max(),np.array([len(i) for i in test]).max())\n",
    "def numpy_fillna(data, width):\n",
    "    # Get lengths of each row of data\n",
    "    lens = np.array([len(i) for i in data])\n",
    "\n",
    "    # Mask of valid places in each row\n",
    "    mask = np.arange(width) < lens[:,None]\n",
    "\n",
    "    # Setup output array and put elements from data into masked positions\n",
    "    out = np.zeros(mask.shape, dtype=data.dtype)\n",
    "    out[mask] = np.concatenate(data)\n",
    "    return out\n",
    "train_pad = numpy_fillna(np.array(train), max_width)\n",
    "test_pad = numpy_fillna(np.array(test), max_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "CPU times: user 13.6 s, sys: 5.58 s, total: 19.2 s\n",
      "Wall time: 21.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import svm, metrics\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(train_pad, train_labels)\n",
    "predict = clf.predict(test_pad)\n",
    "print metrics.accuracy_score(predict, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "CPU times: user 12.3 s, sys: 3.19 s, total: 15.5 s\n",
      "Wall time: 16.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(train_pad, train_labels)\n",
    "predict = clf.predict(test_pad)\n",
    "print metrics.accuracy_score(predict, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.166666666667\n",
      "CPU times: user 4.5 s, sys: 3.23 s, total: 7.73 s\n",
      "Wall time: 8.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import ensemble\n",
    "clf = ensemble.RandomForestClassifier()\n",
    "clf.fit(train_pad, train_labels)\n",
    "predict = clf.predict(test_pad)\n",
    "print metrics.accuracy_score(predict, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.166666666667\n",
      "CPU times: user 16.1 s, sys: 3.89 s, total: 20 s\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()\n",
    "clf.fit(train_pad, train_labels)\n",
    "predict = clf.predict(test_pad)\n",
    "print metrics.accuracy_score(predict, test_labels)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
