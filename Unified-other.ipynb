{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get necessary modules, and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, cv2, sys\n",
    "import numpy as np\n",
    "from scipy import sqrt, pi, arctan2, cos, sin\n",
    "from scipy.ndimage import uniform_filter\n",
    "from sklearn import svm, metrics, tree, ensemble, linear_model\n",
    "\n",
    "# The dimensions to resize video frames to before calculating HoF\n",
    "FIXED_WIDTH = 128\n",
    "FIXED_HEIGHT = 128\n",
    "\n",
    "# Property useful for finding length (in frames) of a video.\n",
    "# Renamed here because the way to access it can differ between machines, so we can change it easily\n",
    "FRAME_LENGTH_PROP = cv2.CAP_PROP_FRAME_COUNT\n",
    "\n",
    "# Threshold ratio for segmentation. Segments longer than this * length of smallest video are not discarded\n",
    "SEG_THRESHOLD = 0.5\n",
    "\n",
    "# Ratio of training data to testing data\n",
    "TRAIN_RATIO = 0.7\n",
    "\n",
    "# level parameter for pyramiding HoF\n",
    "LEVEL = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some setup necessary for all/most descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nc: 100\n",
      "offset: 70\n",
      "train files: 140\n",
      "train labels: 140\n",
      "test files: 60\n",
      "test labels: 60\n"
     ]
    }
   ],
   "source": [
    "# Directory and files of the first action\n",
    "ACT1_DIR = \"./running/\"\n",
    "ACT1_FILES = os.listdir(ACT1_DIR)\n",
    "ACT1_FILES = [ACT1_DIR + f for f in ACT1_FILES]\n",
    "\n",
    "# Directory and files of the second action\n",
    "ACT2_DIR = \"./walking/\"\n",
    "ACT2_FILES = os.listdir(ACT2_DIR)\n",
    "ACT2_FILES = [ACT2_DIR + f for f in ACT2_FILES]\n",
    "\n",
    "# Use equal number of data from each class\n",
    "nc = min(len(ACT1_FILES), len(ACT2_FILES))\n",
    "print \"nc:\", nc\n",
    "ACT1_FILES = ACT1_FILES[0:nc]\n",
    "ACT2_FILES = ACT2_FILES[0:nc]\n",
    "\n",
    "offset = int(np.floor(nc*TRAIN_RATIO))\n",
    "print \"offset:\", offset\n",
    "\n",
    "# Split test and training at ratio\n",
    "train_files = ACT1_FILES[0:offset] + ACT2_FILES[0:offset]\n",
    "test_files = ACT1_FILES[offset:nc] + ACT2_FILES[offset:nc]\n",
    "\n",
    "# Put the labels in vectors\n",
    "train_labels = np.zeros(offset*2, int)\n",
    "train_labels[0:offset] = 1\n",
    "train_labels[offset:offset*2] = 2\n",
    "\n",
    "test_len = nc-offset\n",
    "test_labels = np.zeros(test_len*2, int)\n",
    "test_labels[0:test_len] = 1\n",
    "test_labels[test_len:test_len*2] = 2\n",
    "\n",
    "print \"train files:\", len(train_files)\n",
    "print \"train labels:\", len(train_labels)\n",
    "print \"test files:\", len(test_files)\n",
    "print \"test labels:\", len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a method and parameters for normalizing frames, by transforming them to fixed dimensions and putting them in grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalizeFrame(frame_original):\n",
    "    frame_gray = cv2.cvtColor(frame_original,cv2.COLOR_BGR2GRAY)\n",
    "    frame_gray_resized = cv2.resize(frame_gray, (FIXED_WIDTH, FIXED_HEIGHT))\n",
    "    return frame_gray_resized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optical flow for this project. Uses the Farneback method for calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gets the optical flow [<dx,dy>] from two frames\n",
    "def getOpticalFlow(imPrev, imNew):\n",
    "    flow = cv2.calcOpticalFlowFarneback(imPrev, imNew, flow=None, pyr_scale=.5, levels=3, winsize=9, \n",
    "                                        iterations=1, poly_n=3, poly_sigma=1.1, \n",
    "                                        flags=cv2.OPTFLOW_FARNEBACK_GAUSSIAN)\n",
    "    return flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define HoF, which takes as a parameter the optical flow between two frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the Histogram of Optical Flow (HoF) from the given optical flow\n",
    "def hof(flow, orientations=9, pixels_per_cell=(10, 10),\n",
    "        cells_per_block=(4, 3), normalise=False, motion_threshold=1.):\n",
    "    flow = np.atleast_2d(flow)\n",
    "\n",
    "    if flow.ndim < 3:\n",
    "        raise ValueError(\"Requires dense flow in both directions\")\n",
    "\n",
    "    if normalise:\n",
    "        flow = sqrt(flow)\n",
    "\n",
    "    if flow.dtype.kind == 'u':\n",
    "        flow = flow.astype('float')\n",
    "\n",
    "    gx = np.zeros(flow.shape[:2])\n",
    "    gy = np.zeros(flow.shape[:2])\n",
    "\n",
    "    gx = flow[:,:,1]\n",
    "    gy = flow[:,:,0]\n",
    "\n",
    "    magnitude = sqrt(gx**2 + gy**2)\n",
    "    orientation = arctan2(gy, gx) * (180 / pi) % 180\n",
    "\n",
    "    sy, sx = flow.shape[:2]\n",
    "    cx, cy = pixels_per_cell\n",
    "    bx, by = cells_per_block\n",
    "\n",
    "    n_cellsx = int(np.floor(sx // cx))\n",
    "    n_cellsy = int(np.floor(sy // cy))\n",
    "\n",
    "    orientation_histogram = np.zeros((n_cellsy, n_cellsx, orientations))\n",
    "    subsample = np.index_exp[cy / 2:cy * n_cellsy:cy, cx / 2:cx * n_cellsx:cx]\n",
    "    for i in range(orientations-1):\n",
    "        temp_ori = np.where(orientation < 180 / orientations * (i + 1),\n",
    "                            orientation, -1)\n",
    "        temp_ori = np.where(orientation >= 180 / orientations * i,\n",
    "                            temp_ori, -1)\n",
    "\n",
    "        cond2 = (temp_ori > -1) * (magnitude > motion_threshold)\n",
    "        temp_mag = np.where(cond2, magnitude, 0)\n",
    "\n",
    "        temp_filt = uniform_filter(temp_mag, size=(cy, cx))\n",
    "        orientation_histogram[:, :, i] = temp_filt[subsample]\n",
    "\n",
    "    temp_mag = np.where(magnitude <= motion_threshold, magnitude, 0)\n",
    "\n",
    "    temp_filt = uniform_filter(temp_mag, size=(cy, cx))\n",
    "    orientation_histogram[:, :, -1] = temp_filt[subsample]\n",
    "\n",
    "    n_blocksx = (n_cellsx - bx) + 1\n",
    "    n_blocksy = (n_cellsy - by) + 1\n",
    "    normalised_blocks = np.zeros((n_blocksy, n_blocksx,\n",
    "                                  by, bx, orientations))\n",
    "\n",
    "    for x in range(n_blocksx):\n",
    "        for y in range(n_blocksy):\n",
    "            block = orientation_histogram[y:y+by, x:x+bx, :]\n",
    "            eps = 1e-5\n",
    "            normalised_blocks[y, x, :] = block / sqrt(block.sum()**2 + eps)\n",
    "\n",
    "    return normalised_blocks.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a method for calculating HoF between two frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the Histogram of Optical Flow from two images\n",
    "def getHoF(frame1, frame2):\n",
    "    flow = getOpticalFlow(frame1, frame2)\n",
    "    return hof(flow, pixels_per_cell=(20,20), cells_per_block=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And another for calculating the HoF feature vector for an entire video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the Histogram of Optical Flows of a video grouped sequentially in a 1D array\n",
    "def getSequentialHoF(video_path):\n",
    "    hofs = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret1, frame1 = cap.read()\n",
    "    frame1 = normalizeFrame(frame1)\n",
    "    while(cap.isOpened()):\n",
    "        ret2, frame2 = cap.read()\n",
    "        if ret2 == True:\n",
    "            frame2 = normalizeFrame(frame2)\n",
    "            hof_array = getHoF(frame1, frame2)\n",
    "            hofs = np.concatenate((hofs, hof_array),axis=0)\n",
    "            frame1 = frame2\n",
    "        else:\n",
    "            break\n",
    "    return hofs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have all the methods necessary for a base HoF descriptor; however, having training/testing videos of different lengths means our feature vectors will have different lengths (not good for classifiers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach to remedy this is to pad shorter vectors with zeros to the length of the largest vector. We define two methods to help us with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the length of the largest row in training set and testing set\n",
    "def maxRow(test, train):\n",
    "    return max(np.array([len(i) for i in train]).max(),\n",
    "               np.array([len(i) for i in test]).max())\n",
    "\n",
    "# Pad each row of the 2D array, with 0, to a specified width\n",
    "def numpy_fillna(data, width):\n",
    "    # Get lengths of each row of data\n",
    "    lens = np.array([len(i) for i in data])\n",
    "\n",
    "    # Mask of valid places in each row\n",
    "    mask = np.arange(width) < lens[:,None]\n",
    "\n",
    "    # Setup output array and put elements from data into masked positions\n",
    "    out = np.zeros(mask.shape, dtype=data.dtype)\n",
    "    out[mask] = np.concatenate(data)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a method which gets all the feature vectors we need, ensuring they are padded to the right length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeatures_Baseline(train_files, train_labels, test_files, test_labels):\n",
    "    # Make feature vectors\n",
    "    train = [getSequentialHoF(p) for p in train_files]\n",
    "    test = [getSequentialHoF(p) for p in test_files]\n",
    "    \n",
    "    # Pad them to the max video width\n",
    "    max_width = maxRow(test, train);\n",
    "    train_pad = numpy_fillna(np.array(train), max_width)\n",
    "    test_pad = numpy_fillna(np.array(test), max_width)\n",
    "    \n",
    "    return train_pad, train_labels, test_pad, test_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach works, but may not give us the best accuracy since it introduces noise, especially for shorter videos. A different approach we take is to trim the videos to the length of the smallest video and then apply HoF, using frames only from the middle of each video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a method to find the length of the shortest video in the video directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the length in frames of the shortest video in the provided dataset\n",
    "def shortest(data_dir):\n",
    "    # get list of files in the directory. directory should be flat with only video files in it\n",
    "    files = os.listdir(data_dir)\n",
    "    \n",
    "    # Find the length of the shortest video (in frames)\n",
    "    shortestLen = sys.maxint\n",
    "    for i in range(len(files)):\n",
    "        cap = cv2.VideoCapture(data_dir+files[i])\n",
    "        length = int(cap.get(FRAME_LENGTH_PROP))\n",
    "        if length < shortestLen:\n",
    "            shortestLen = length\n",
    "            \n",
    "        cap.release()\n",
    "        \n",
    "    return shortestLen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a method which gets the HoF feature vector from the middle of a video, using the amount of frames determined by the above method (passed as param). This is the same as the base sequential HoF method, modified to get frames from the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the Histogram of Optical Flows of a video grouped sequentially in a 1D array\n",
    "# Use only the specified amount of frames, from the middle of the video\n",
    "def getSequentialHoFMiddle(video_path, frames):\n",
    "    hofs = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    length = int(cap.get(FRAME_LENGTH_PROP))\n",
    "    startIdx = ((length - 1) - frames)/2\n",
    "    if startIdx < 1:\n",
    "        startIdx = 0\n",
    "        \n",
    "    # skip through beginning unneeded frames\n",
    "    frameNum = 0\n",
    "    while (frameNum < startIdx):\n",
    "        cap.grab()\n",
    "        frameNum += 1\n",
    "    frameNum = 0\n",
    "    \n",
    "    # Calculate HoF from necessary frames\n",
    "    ret1, frame1 = cap.read()\n",
    "    frame1 = normalizeFrame(frame1)\n",
    "    while(frameNum < frames-2):\n",
    "        ret2, frame2 = cap.read()\n",
    "        if ret2 == True:\n",
    "            frame2 = normalizeFrame(frame2)\n",
    "            hof_array = getHoF(frame1, frame2)\n",
    "            hofs = np.concatenate((hofs, hof_array),axis=0)\n",
    "            frame1 = frame2\n",
    "            frameNum += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    return hofs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a method which gets all the feature vectors we need, ensuring they are made from trimmed video middles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeatures_Trimmed(train_files, train_labels, test_files, test_labels):\n",
    "    numFrames = min(shortest(ACT1_DIR), shortest(ACT2_DIR))\n",
    "    train = [getSequentialHoFMiddle(p, numFrames) for p in train_files]\n",
    "    test = [getSequentialHoFMiddle(p, numFrames) for p in test_files]\n",
    "    \n",
    "    return np.array(train), train_labels, np.array(test), test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us feature vectors with less noise than the baseline method, but can also eliminate a lot of useful information from longer videos. Another method which preserves most of this information while also ensuring uniform-length vectors, is to break each video into fixed-length segments, where each segment is the length of the shortest video in the set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define another method for calculating HoF feature vectors on each segment of a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSequentialHoFSegments(video_path, label, frames):\n",
    "    seg_hofs = []\n",
    "    hofs = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    length = int(cap.get(FRAME_LENGTH_PROP))\n",
    "    \n",
    "    ret1, frame1 = cap.read()\n",
    "    frame1 = normalizeFrame(frame1)\n",
    "    while(cap.isOpened()):\n",
    "        hofs = []\n",
    "        for i in range(frames-1):\n",
    "            ret2, frame2 = cap.read()\n",
    "            if ret2 == True:\n",
    "                frame2 = normalizeFrame(frame2)\n",
    "                hof_array = getHoF(frame1, frame2)\n",
    "                hofs = np.concatenate((hofs, hof_array),axis=0)\n",
    "                frame1 = frame2\n",
    "            else:\n",
    "                cap.release()\n",
    "                break\n",
    "        seg_hofs.append(hofs)\n",
    "    \n",
    "    seg_labels = np.full(len(seg_hofs), label)\n",
    "    return seg_hofs, seg_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above method can produce one segment per video which is shorter than the minimum length (from the end). We could pad these, but for short segments this would introduce a lot of noise. We define a threshold (is a constant defined at top) length for these videos, pad those above the threshold, and discard segments which are too short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeatures_Segmented(train_files, train_labels, test_files, test_labels):\n",
    "    numFrames = min(shortest(ACT1_DIR), shortest(ACT2_DIR))\n",
    "    train_result = [a for t,l in zip(train_files, train_labels) for a in getSequentialHoFSegments(t, l, numFrames)]\n",
    "    test_result = [a for t,l in zip(test_files, test_labels) for a in getSequentialHoFSegments(t, l, numFrames)]\n",
    "    train = [y for x in train_result[::2] for y in x]\n",
    "    new_train_labels = np.concatenate(train_result[1::2])\n",
    "    test = [y for x in test_result[::2] for y in x]\n",
    "    new_test_labels = np.concatenate(test_result[1::2])\n",
    "    train_threshold = int(max([len(x) for x in train]) * SEG_THRESHOLD)\n",
    "    test_threshold = int(max([len(x) for x in test]) * SEG_THRESHOLD)\n",
    "    train_trimmed = [x for x,y in zip(train,new_train_labels) if len(x) > train_threshold]\n",
    "    train_labels_trimmed = [y for x,y in zip(train,new_train_labels) if len(x) > train_threshold]\n",
    "    test_trimmed = [x for x,y in zip(test,new_test_labels) if len(x) > test_threshold]\n",
    "    test_labels_trimmed = [y for x,y in zip(test,new_test_labels) if len(x) > test_threshold]\n",
    "    max_width = max(np.array([len(i) for i in test_trimmed]).max(),np.array([len(i) for i in test_trimmed]).max())\n",
    "    train_pad = numpy_fillna(np.array(train_trimmed), max_width)\n",
    "    test_pad = numpy_fillna(np.array(test_trimmed), max_width)\n",
    "    \n",
    "    return train_pad, train_labels_trimmed, test_pad, test_labels_trimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now defined how to get features for HoF, using the padding, trimming, and segmentation methods of ensuring feature vectors are the same shape. Additionally, we want to make HoF feature vectors which use gaussian pyramiding in order to help reduce noise factors in videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a method for getting a pyramid for an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPyramid(img, level):\n",
    "    p = []\n",
    "    p.append(img)\n",
    "    for i in range(level):\n",
    "        p.append(cv2.pyrDown(p[len(p)-1]))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a method for getting pyramid HoF between two frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pyramidHof(frame1, frame2, level, getHofFunc, getPyramidFunc, getOpticalFlowFunc):\n",
    "    if frame1.shape != frame2.shape:\n",
    "        raise ValueError('frame1 and frame2 should have identical dimensions')\n",
    "    pyramid1 = getPyramidFunc(frame1, level)\n",
    "    pyramid2 = getPyramidFunc(frame2, level)\n",
    "    pyramidHoF = []\n",
    "    for i in range(level):\n",
    "        subframe1 = pyramid1[i]\n",
    "        subframe2 = pyramid2[i]\n",
    "        subflow = getOpticalFlowFunc(subframe1, subframe2)\n",
    "        subhof = getHofFunc(subflow)\n",
    "        pyramidHoF = np.append(pyramidHoF, subhof)\n",
    "    return pyramidHoF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a method for determining pyramid HoF on whole video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPyramidHof(video_path, level, normalizeFrameFunc=normalizeFrame, getHofFunc=hof, getPyramidFunc=getPyramid, getOpticalFlowFunc=getOpticalFlow):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    vid_hof = []\n",
    "    ret1, frame1 = cap.read()\n",
    "    frame1 = normalizeFrameFunc(frame1)\n",
    "    while(cap.isOpened()):\n",
    "        ret2, frame2 = cap.read()\n",
    "        if ret2 == True:\n",
    "            frame2 = normalizeFrameFunc(frame2)\n",
    "            biframe_hof = pyramidHof(frame1, frame2, level, getHofFunc, getPyramidFunc, getOpticalFlowFunc)\n",
    "            vid_hof = np.append(vid_hof, biframe_hof)\n",
    "            frame1 = frame2\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "    return vid_hof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a method for getting pyramid HoF feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeatures_Pyramid_Baseline(train_files, train_labels, test_files, test_labels):\n",
    "    # make feature vectors\n",
    "    train = [getPyramidHof(p, LEVEL) for p in train_files]\n",
    "    test = [getPyramidHof(p, LEVEL) for p in test_files]\n",
    "    \n",
    "    # Pad them to the max video width\n",
    "    max_width = maxRow(test, train);\n",
    "    train_pad = numpy_fillna(np.array(train), max_width)\n",
    "    test_pad = numpy_fillna(np.array(test), max_width)\n",
    "    \n",
    "    return train_pad, train_labels, test_pad, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to repeat what we did before with trimming for pyramid HoF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPyramidHofMiddle(video_path, frames, level, normalizeFrameFunc=normalizeFrame, getHofFunc=hof, getPyramidFunc=getPyramid, getOpticalFlowFunc=getOpticalFlow):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    vid_hof = []\n",
    "    \n",
    "    length = int(cap.get(FRAME_LENGTH_PROP))\n",
    "    startIdx = ((length - 1) - frames)/2\n",
    "    if startIdx < 1:\n",
    "        startIdx = 0\n",
    "    # skip through beginning unneeded frames\n",
    "    frameNum = 0\n",
    "    while (frameNum < startIdx):\n",
    "        cap.grab()\n",
    "        frameNum += 1\n",
    "    frameNum = 0\n",
    "    \n",
    "    ret1, frame1 = cap.read()\n",
    "    frame1 = normalizeFrameFunc(frame1)\n",
    "    while(frameNum < frames-2):\n",
    "        ret2, frame2 = cap.read()\n",
    "        if ret2 == True:\n",
    "            frame2 = normalizeFrameFunc(frame2)\n",
    "            biframe_hof = pyramidHof(frame1, frame2, level, getHofFunc, getPyramidFunc, getOpticalFlowFunc)\n",
    "            vid_hof = np.append(vid_hof, biframe_hof)\n",
    "            frame1 = frame2\n",
    "            frameNum += 1\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "    return vid_hof\n",
    "\n",
    "def getFeatures_Pyramid_Trimmed(train_files, train_labels, test_files, test_labels):\n",
    "    numFrames = min(shortest(ACT1_DIR), shortest(ACT2_DIR))\n",
    "    train = [getPyramidHofMiddle(p, numFrames, LEVEL) for p in train_files]\n",
    "    test = [getPyramidHofMiddle(p, numFrames, LEVEL) for p in test_files]\n",
    "    \n",
    "    return np.array(train), train_labels, np.array(test), test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...As well as for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPyramidHoFSegments(video_path, label, frames, level, normalizeFrameFunc=normalizeFrame, getHofFunc=hof, getPyramidFunc=getPyramid, getOpticalFlowFunc=getOpticalFlow):\n",
    "    seg_hofs = []\n",
    "    hofs = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    length = int(cap.get(FRAME_LENGTH_PROP))\n",
    "    \n",
    "    ret1, frame1 = cap.read()\n",
    "    frame1 = normalizeFrame(frame1)\n",
    "    while(cap.isOpened()):\n",
    "        hofs = []\n",
    "        for i in range(frames-1):\n",
    "            ret2, frame2 = cap.read()\n",
    "            if ret2 == True:\n",
    "                frame2 = normalizeFrameFunc(frame2)\n",
    "                biframe_hof = pyramidHof(frame1, frame2, level, getHofFunc, getPyramidFunc, getOpticalFlowFunc)\n",
    "                hofs = np.concatenate((hofs, biframe_hof),axis=0)\n",
    "                frame1 = frame2\n",
    "            else:\n",
    "                cap.release()\n",
    "                break\n",
    "        seg_hofs.append(hofs)\n",
    "    \n",
    "    seg_labels = np.full(len(seg_hofs), label)\n",
    "    return seg_hofs, seg_labels\n",
    "\n",
    "def getFeatures_Pyramid_Segmented(train_files, train_labels, test_files, test_labels):\n",
    "    numFrames = min(shortest(ACT1_DIR), shortest(ACT2_DIR))\n",
    "    train_result = [a for t,l in zip(train_files, train_labels) for a in getPyramidHoFSegments(t, l, numFrames, LEVEL)]\n",
    "    test_result = [a for t,l in zip(test_files, test_labels) for a in getPyramidHoFSegments(t, l, numFrames, LEVEL)]\n",
    "    train = [y for x in train_result[::2] for y in x]\n",
    "    new_train_labels = np.concatenate(train_result[1::2])\n",
    "    test = [y for x in test_result[::2] for y in x]\n",
    "    new_test_labels = np.concatenate(test_result[1::2])\n",
    "    train_threshold = int(max([len(x) for x in train]) * SEG_THRESHOLD)\n",
    "    test_threshold = int(max([len(x) for x in test]) * SEG_THRESHOLD)\n",
    "    train_trimmed = [x for x,y in zip(train,new_train_labels) if len(x) > train_threshold]\n",
    "    train_labels_trimmed = [y for x,y in zip(train,new_train_labels) if len(x) > train_threshold]\n",
    "    test_trimmed = [x for x,y in zip(test,new_test_labels) if len(x) > test_threshold]\n",
    "    test_labels_trimmed = [y for x,y in zip(test,new_test_labels) if len(x) > test_threshold]\n",
    "    max_width = max(np.array([len(i) for i in test_trimmed]).max(),np.array([len(i) for i in test_trimmed]).max())\n",
    "    train_pad = numpy_fillna(np.array(train_trimmed), max_width)\n",
    "    test_pad = numpy_fillna(np.array(test_trimmed), max_width)\n",
    "    \n",
    "    return train_pad, train_labels_trimmed, test_pad, test_labels_trimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to train and evaluate models. Define method for training models given featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_model(classifier, (train_features, train_labels, test_features, test_labels)):\n",
    "    clf = None\n",
    "    name = \"\"\n",
    "    if (classifier == 0):\n",
    "        clf = svm.SVC()\n",
    "        name = \"SVM\"\n",
    "    elif (classifier == 1):\n",
    "        clf = tree.DecisionTreeClassifier()\n",
    "        name = \"Decision Tree\"\n",
    "    elif (classifier == 2):\n",
    "        clf = ensemble.RandomForestClassifier()\n",
    "        name = \"Random Forest\"\n",
    "    else:\n",
    "        clf = linear_model.LogisticRegression()\n",
    "        name = \"Logistic Regression\"\n",
    "    \n",
    "    clf.fit(train_features, train_labels)\n",
    "    predict = clf.predict(test_features)\n",
    "    \n",
    "    return metrics.accuracy_score(predict, test_labels), name\\\n",
    "\n",
    "def run(data):\n",
    "    for i in range(4):\n",
    "        score, name = eval_model(i, data);\n",
    "        print name + \": \" + str(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test base HoF with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 0.833333333333\n",
      "Decision Tree: 0.9\n",
      "Random Forest: 0.916666666667\n",
      "Logistic Regression: 0.933333333333\n"
     ]
    }
   ],
   "source": [
    "data = getFeatures_Baseline(train_files, train_labels, test_files, test_labels)\n",
    "run(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test base HoF with trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 0.7\n",
      "Decision Tree: 0.633333333333\n",
      "Random Forest: 0.716666666667\n",
      "Logistic Regression: 0.766666666667\n"
     ]
    }
   ],
   "source": [
    "data = getFeatures_Trimmed(train_files, train_labels, test_files, test_labels)\n",
    "run(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test base HoF with segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 0.638709677419\n",
      "Decision Tree: 0.741935483871\n",
      "Random Forest: 0.787096774194\n",
      "Logistic Regression: 0.806451612903\n"
     ]
    }
   ],
   "source": [
    "data = getFeatures_Segmented(train_files, train_labels, test_files, test_labels)\n",
    "run(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test pyramid HoF with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getFeatures_Pyramid_Baseline(train_files, train_labels, test_files, test_labels)\n",
    "run(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test pyramid HoF with trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getFeatures_Pyramid_Trimmed(train_files, train_labels, test_files, test_labels)\n",
    "run(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test pyramid HoF with segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = getFeatures_Pyramid_Segmented(train_files, train_labels, test_files, test_labels)\n",
    "run(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
