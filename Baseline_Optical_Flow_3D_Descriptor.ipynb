{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Gets the optical flow [<dx,dy>] from two frames\n",
    "def getOpticalFlow(imPrev, imNew):\n",
    "    flow = cv2.calcOpticalFlowFarneback(imPrev, imNew, flow=None, pyr_scale=.5, levels=3, winsize=9, iterations=1, poly_n=3, poly_sigma=1.1, flags=cv2.OPTFLOW_FARNEBACK_GAUSSIAN)\n",
    "    return flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Util functions to plot two images side by side\n",
    "def show2images(im1, im2, title1=\"Image 1\", title2=\"image 2\"):\n",
    "    plt.subplot(1,2,1),plt.imshow(im1, cmap=\"gray\"),plt.title(title1),plt.xticks([]),plt.yticks([])\n",
    "    plt.subplot(1,2,2),plt.imshow(im2, cmap=\"gray\"),plt.title(title2),plt.xticks([]),plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Testing capturing frames from videos via CV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "SOURCE = \"sample.avi\"\n",
    "cap = cv2.VideoCapture(SOURCE)\n",
    "ret1, frame1_original = cap.read()\n",
    "ret2, frame2_original = cap.read()\n",
    "cap.release()\n",
    "\n",
    "print(ret1)\n",
    "print(ret2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We are standardizing the images so they are all grayscaled and have the same width and height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "/build/opencv/src/opencv-3.2.0/modules/imgproc/src/color.cpp:9748: error: (-215) scn == 3 || scn == 4 in function cvtColor\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-19c498591c07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mFIXED_HEIGHT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mframe1_gray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe1_original\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mframe2_gray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe2_original\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /build/opencv/src/opencv-3.2.0/modules/imgproc/src/color.cpp:9748: error: (-215) scn == 3 || scn == 4 in function cvtColor\n"
     ]
    }
   ],
   "source": [
    "FIXED_WIDTH = 160\n",
    "FIXED_HEIGHT = 120\n",
    "\n",
    "frame1_gray = cv2.cvtColor(frame1_original,cv2.COLOR_BGR2GRAY)\n",
    "frame2_gray = cv2.cvtColor(frame2_original,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "frame1_gray_resized = cv2.resize(frame1_gray, (FIXED_WIDTH, FIXED_HEIGHT))\n",
    "frame2_gray_resized = cv2.resize(frame1_gray, (FIXED_WIDTH, FIXED_HEIGHT))\n",
    "\n",
    "show2images(frame1_gray_resized, frame2_gray_resized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Calculate the optical flow of the two standardized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flow = getOpticalFlow(frame1_gray_resized, frame2_gray_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Compute the Histogram of Optical Flow (HoF) from the given optica flow\n",
    "\n",
    "from scipy import sqrt, pi, arctan2, cos, sin\n",
    "from scipy.ndimage import uniform_filter\n",
    "def hof(flow, orientations=9, pixels_per_cell=(10, 10),\n",
    "        cells_per_block=(4, 3), visualise=False, normalise=False, motion_threshold=1.):\n",
    "\n",
    "    \"\"\"Extract Histogram of Optical Flow (HOF) for a given image.\n",
    "    Key difference between this and HOG is that flow is MxNx2 instead of MxN\n",
    "    Compute a Histogram of Optical Flow (HOF) by\n",
    "        1. (optional) global image normalisation\n",
    "        2. computing the dense optical flow\n",
    "        3. computing flow histograms\n",
    "        4. normalising across blocks\n",
    "        5. flattening into a feature vector\n",
    "    Parameters\n",
    "    ----------\n",
    "    Flow : (M, N) ndarray\n",
    "        Input image (x and y flow images).\n",
    "    orientations : int\n",
    "        Number of orientation bins.\n",
    "    pixels_per_cell : 2 tuple (int, int)\n",
    "        Size (in pixels) of a cell.\n",
    "    cells_per_block  : 2 tuple (int,int)\n",
    "        Number of cells in each block.\n",
    "    visualise : bool, optional\n",
    "        Also return an image of the hof.\n",
    "    normalise : bool, optional\n",
    "        Apply power law compression to normalise the image before\n",
    "        processing.\n",
    "    static_threshold : threshold for no motion\n",
    "    Returns\n",
    "    -------\n",
    "    newarr : ndarray\n",
    "        hof for the image as a 1D (flattened) array.\n",
    "    hof_image : ndarray (if visualise=True)\n",
    "        A visualisation of the hof image.\n",
    "    References\n",
    "    ----------\n",
    "    * http://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\n",
    "    * Dalal, N and Triggs, B, Histograms of Oriented Gradients for\n",
    "      Human Detection, IEEE Computer Society Conference on Computer\n",
    "      Vision and Pattern Recognition 2005 San Diego, CA, USA\n",
    "    \"\"\"\n",
    "    flow = np.atleast_2d(flow)\n",
    "\n",
    "    \"\"\" \n",
    "    -1-\n",
    "    The first stage applies an optional global image normalisation\n",
    "    equalisation that is designed to reduce the influence of illumination\n",
    "    effects. In practice we use gamma (power law) compression, either\n",
    "    computing the square root or the log of each colour channel.\n",
    "    Image texture strength is typically proportional to the local surface\n",
    "    illumination so this compression helps to reduce the effects of local\n",
    "    shadowing and illumination variations.\n",
    "    \"\"\"\n",
    "\n",
    "    if flow.ndim < 3:\n",
    "        raise ValueError(\"Requires dense flow in both directions\")\n",
    "\n",
    "    if normalise:\n",
    "        flow = sqrt(flow)\n",
    "\n",
    "    \"\"\" \n",
    "    -2-\n",
    "    The second stage computes first order image gradients. These capture\n",
    "    contour, silhouette and some texture information, while providing\n",
    "    further resistance to illumination variations. The locally dominant\n",
    "    colour channel is used, which provides colour invariance to a large\n",
    "    extent. Variant methods may also include second order image derivatives,\n",
    "    which act as primitive bar detectors - a useful feature for capturing,\n",
    "    e.g. bar like structures in bicycles and limbs in humans.\n",
    "    \"\"\"\n",
    "\n",
    "    if flow.dtype.kind == 'u':\n",
    "        # convert uint image to float\n",
    "        # to avoid problems with subtracting unsigned numbers in np.diff()\n",
    "        flow = flow.astype('float')\n",
    "\n",
    "    gx = np.zeros(flow.shape[:2])\n",
    "    gy = np.zeros(flow.shape[:2])\n",
    "    # gx[:, :-1] = np.diff(flow[:,:,1], n=1, axis=1)\n",
    "    # gy[:-1, :] = np.diff(flow[:,:,0], n=1, axis=0)\n",
    "\n",
    "    gx = flow[:,:,1]\n",
    "    gy = flow[:,:,0]\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\" \n",
    "    -3-\n",
    "    The third stage aims to produce an encoding that is sensitive to\n",
    "    local image content while remaining resistant to small changes in\n",
    "    pose or appearance. The adopted method pools gradient orientation\n",
    "    information locally in the same way as the SIFT [Lowe 2004]\n",
    "    feature. The image window is divided into small spatial regions,\n",
    "    called \"cells\". For each cell we accumulate a local 1-D histogram\n",
    "    of gradient or edge orientations over all the pixels in the\n",
    "    cell. This combined cell-level 1-D histogram forms the basic\n",
    "    \"orientation histogram\" representation. Each orientation histogram\n",
    "    divides the gradient angle range into a fixed number of\n",
    "    predetermined bins. The gradient magnitudes of the pixels in the\n",
    "    cell are used to vote into the orientation histogram.\n",
    "    \"\"\"\n",
    "\n",
    "    magnitude = sqrt(gx**2 + gy**2)\n",
    "    orientation = arctan2(gy, gx) * (180 / pi) % 180\n",
    "\n",
    "    sy, sx = flow.shape[:2]\n",
    "    cx, cy = pixels_per_cell\n",
    "    bx, by = cells_per_block\n",
    "\n",
    "    n_cellsx = int(np.floor(sx // cx))  # number of cells in x\n",
    "    n_cellsy = int(np.floor(sy // cy))  # number of cells in y\n",
    "\n",
    "    # compute orientations integral images\n",
    "    orientation_histogram = np.zeros((n_cellsy, n_cellsx, orientations))\n",
    "    subsample = np.index_exp[cy / 2:cy * n_cellsy:cy, cx / 2:cx * n_cellsx:cx]\n",
    "    for i in range(orientations-1):\n",
    "        #create new integral image for this orientation\n",
    "        # isolate orientations in this range\n",
    "\n",
    "        temp_ori = np.where(orientation < 180 / orientations * (i + 1),\n",
    "                            orientation, -1)\n",
    "        temp_ori = np.where(orientation >= 180 / orientations * i,\n",
    "                            temp_ori, -1)\n",
    "        # select magnitudes for those orientations\n",
    "        cond2 = (temp_ori > -1) * (magnitude > motion_threshold)\n",
    "        temp_mag = np.where(cond2, magnitude, 0)\n",
    "\n",
    "        temp_filt = uniform_filter(temp_mag, size=(cy, cx))\n",
    "        orientation_histogram[:, :, i] = temp_filt[subsample]\n",
    "\n",
    "    ''' Calculate the no-motion bin '''\n",
    "    temp_mag = np.where(magnitude <= motion_threshold, magnitude, 0)\n",
    "\n",
    "    temp_filt = uniform_filter(temp_mag, size=(cy, cx))\n",
    "    orientation_histogram[:, :, -1] = temp_filt[subsample]\n",
    "\n",
    "    # now for each cell, compute the histogram\n",
    "    hof_image = None\n",
    "\n",
    "    if visualise:\n",
    "        from skimage import draw\n",
    "\n",
    "        radius = min(cx, cy) // 2 - 1\n",
    "        hof_image = np.zeros((sy, sx), dtype=float)\n",
    "        for x in range(n_cellsx):\n",
    "            for y in range(n_cellsy):\n",
    "                for o in range(orientations-1):\n",
    "                    centre = tuple([y * cy + cy // 2, x * cx + cx // 2])\n",
    "                    dx = int(radius * cos(float(o) / orientations * np.pi))\n",
    "                    dy = int(radius * sin(float(o) / orientations * np.pi))\n",
    "                    rr, cc = draw.bresenham(centre[0] - dy, centre[1] - dx,\n",
    "                                            centre[0] + dy, centre[1] + dx)\n",
    "                    hof_image[rr, cc] += orientation_histogram[y, x, o]\n",
    "\n",
    "    \"\"\"\n",
    "    The fourth stage computes normalisation, which takes local groups of\n",
    "    cells and contrast normalises their overall responses before passing\n",
    "    to next stage. Normalisation introduces better invariance to illumination,\n",
    "    shadowing, and edge contrast. It is performed by accumulating a measure\n",
    "    of local histogram \"energy\" over local groups of cells that we call\n",
    "    \"blocks\". The result is used to normalise each cell in the block.\n",
    "    Typically each individual cell is shared between several blocks, but\n",
    "    its normalisations are block dependent and thus different. The cell\n",
    "    thus appears several times in the final output vector with different\n",
    "    normalisations. This may seem redundant but it improves the performance.\n",
    "    We refer to the normalised block descriptors as Histogram of Oriented\n",
    "    Gradient (hog) descriptors.\n",
    "    \"\"\"\n",
    "\n",
    "    n_blocksx = (n_cellsx - bx) + 1\n",
    "    n_blocksy = (n_cellsy - by) + 1\n",
    "    normalised_blocks = np.zeros((n_blocksy, n_blocksx,\n",
    "                                  by, bx, orientations))\n",
    "\n",
    "    for x in range(n_blocksx):\n",
    "        for y in range(n_blocksy):\n",
    "            block = orientation_histogram[y:y+by, x:x+bx, :]\n",
    "            eps = 1e-5\n",
    "            normalised_blocks[y, x, :] = block / sqrt(block.sum()**2 + eps)\n",
    "\n",
    "    \"\"\"\n",
    "    The final step collects the hof descriptors from all blocks of a dense\n",
    "    overlapping grid of blocks covering the detection window into a combined\n",
    "    feature vector for use in the window classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    if visualise:\n",
    "        return normalised_blocks.ravel(), hof_image\n",
    "    else:\n",
    "        return normalised_blocks.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800,)\n",
      "[ 0.          0.          0.         ...,  0.          0.          0.20134845]\n"
     ]
    }
   ],
   "source": [
    "hof_result = hof(flow, pixels_per_cell=(20,20), cells_per_block=(5,5))\n",
    "print hof_result.shape\n",
    "print hof_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nc: 232\n",
      "offset: 208\n",
      "train files: 416\n",
      "train labels: 416\n",
      "test files: 48\n",
      "test labels: 48\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import os\n",
    "\n",
    "# Collect the file path of all the running and walking videos, we will only be using these 2 classes\n",
    "RUN_DIR = \"hof/run/\"\n",
    "RUN_FILES = os.listdir(RUN_DIR)\n",
    "RUN_FILES = [RUN_DIR + f for f in RUN_FILES]\n",
    "WALK_DIR = \"hof/walk/\"\n",
    "WALK_FILES = os.listdir(WALK_DIR)\n",
    "WALK_FILES = [WALK_DIR + f for f in WALK_FILES]\n",
    "\n",
    "# Use equal number of data from each class\n",
    "nc = min(len(RUN_FILES), len(WALK_FILES))\n",
    "print \"nc:\", nc\n",
    "RUN_FILES = RUN_FILES[0:nc]\n",
    "WALK_FILES = WALK_FILES[0:nc]\n",
    "\n",
    "RATIO = 0.9\n",
    "offset = int(np.floor(nc*RATIO))\n",
    "print \"offset:\", offset\n",
    "\n",
    "# Split test and training at a ratio of 1:9\n",
    "train_files = RUN_FILES[0:offset] + WALK_FILES[0:offset]\n",
    "test_files = RUN_FILES[offset:nc] + WALK_FILES[offset:nc]\n",
    "\n",
    "# Put the labels in vectors\n",
    "train_labels = np.zeros(offset*2, int)\n",
    "train_labels[0:offset] = 1 #RUN=1\n",
    "train_labels[offset:offset*2] = 2 #WALK=2\n",
    "\n",
    "test_len = nc-offset\n",
    "test_labels = np.zeros(test_len*2, int)\n",
    "test_labels[0:test_len] = 1 #RUN=1\n",
    "test_labels[test_len:test_len*2] = 2 #WALK=2\n",
    "\n",
    "print \"train files:\", len(train_files)\n",
    "print \"train labels:\", len(train_labels)\n",
    "print \"test files:\", len(test_files)\n",
    "print \"test labels:\", len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Standardize the frames to gray-scale and equal dimensions\n",
    "\n",
    "FIXED_WIDTH = 160\n",
    "FIXED_HEIGHT = 120\n",
    "def normalizeFrame(frame_original):\n",
    "    frame_gray = cv2.cvtColor(frame_original,cv2.COLOR_BGR2GRAY)\n",
    "    frame_gray_resized = cv2.resize(frame_gray, (FIXED_WIDTH, FIXED_HEIGHT))\n",
    "    return frame_gray_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get the Histogram of Optical Flow from two images\n",
    "def getHoF(frame1, frame2):\n",
    "    flow = getOpticalFlow(frame1, frame2)\n",
    "    return hof(flow, pixels_per_cell=(20,20), cells_per_block=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get the Histogram of Optical Flows of a video grouped sequentially in a 1D array\n",
    "def getSequentialHoF(video_path):\n",
    "    hofs = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret1, frame1 = cap.read()\n",
    "    frame1 = normalizeFrame(frame1)\n",
    "    while(cap.isOpened()):\n",
    "        ret2, frame2 = cap.read()\n",
    "        if ret2 == True:\n",
    "            frame2 = normalizeFrame(frame2)\n",
    "            hof_array = getHoF(frame1, frame2)\n",
    "            hofs = np.concatenate((hofs, hof_array),axis=0)\n",
    "#             show2images(frame1, frame2)\n",
    "            frame1 = frame2\n",
    "        else:\n",
    "            break\n",
    "    return hofs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Construct the training set and test set, which are arrays of sequential HoF for each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = [getSequentialHoF(p) for p in train_files]\n",
    "test = [getSequentialHoF(p) for p in test_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The problem is, the rows have varying lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131400\n",
      "86400\n"
     ]
    }
   ],
   "source": [
    "print len(test[1])\n",
    "print len(test[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We find the row with the largest length in training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "max_width = max(np.array([len(i) for i in train]).max(),np.array([len(i) for i in test]).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This method pads each row of the 2D array, with 0, to a specified width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def numpy_fillna(data, width):\n",
    "    # Get lengths of each row of data\n",
    "    lens = np.array([len(i) for i in data])\n",
    "\n",
    "    # Mask of valid places in each row\n",
    "    mask = np.arange(width) < lens[:,None]\n",
    "\n",
    "    # Setup output array and put elements from data into masked positions\n",
    "    out = np.zeros(mask.shape, dtype=data.dtype)\n",
    "    out[mask] = np.concatenate(data)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We are changing every row length to the maximum row length in testing and training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_pad = numpy_fillna(np.array(train), max_width)\n",
    "test_pad = numpy_fillna(np.array(test), max_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create an SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(train_pad, train_labels)\n",
    "predict = clf.predict(test_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Get the accuracy of the SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60416666666666663"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(predict, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54166666666666663"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "tree_clf = tree.DecisionTreeClassifier()\n",
    "tree_clf.fit(train_pad, train_labels)\n",
    "predict = tree_clf.predict(test_pad)\n",
    "metrics.accuracy_score(predict, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "rf_clf = ensemble.RandomForestClassifier()\n",
    "rf_clf.fit(train_pad, train_labels)\n",
    "predict = rf_clf.predict(test_pad)\n",
    "metrics.accuracy_score(predict, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5625"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "lr_clf = linear_model.LogisticRegression()\n",
    "lr_clf.fit(train_pad, train_labels)\n",
    "predict = lr_clf.predict(test_pad)\n",
    "metrics.accuracy_score(predict, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
