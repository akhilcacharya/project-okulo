{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gets the optical flow [<dx,dy>] from two frames\n",
    "def getOpticalFlow(imPrev, imNew):\n",
    "    flow = cv2.calcOpticalFlowFarneback(imPrev, imNew, flow=None, pyr_scale=.5, levels=3, winsize=9, iterations=1, poly_n=3, poly_sigma=1.1, flags=cv2.OPTFLOW_FARNEBACK_GAUSSIAN)\n",
    "    return flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Util functions to plot two images side by side\n",
    "def show2images(im1, im2, title1=\"Image 1\", title2=\"image 2\"):\n",
    "    plt.subplot(1,2,1),plt.imshow(im1, cmap=\"gray\"),plt.title(title1),plt.xticks([]),plt.yticks([])\n",
    "    plt.subplot(1,2,2),plt.imshow(im2, cmap=\"gray\"),plt.title(title2),plt.xticks([]),plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing capturing frames from videos via CV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "/Users/akhilacharya/Documents/Coding/Classes/CSC422/Project/project-okulo\n"
     ]
    }
   ],
   "source": [
    "SOURCE = \"sample.mp4\"\n",
    "cap = cv2.VideoCapture(SOURCE)\n",
    "ret1, frame1_original = cap.read()\n",
    "ret2, frame2_original = cap.read()\n",
    "cap.release()\n",
    "\n",
    "print ret1\n",
    "print ret2\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "print os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are standardizing the images so they are all grayscaled and have the same width and height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FIXED_WIDTH = 160\n",
    "FIXED_HEIGHT = 120\n",
    "\n",
    "frame1_gray = cv2.cvtColor(frame1_original,cv2.COLOR_BGR2GRAY)\n",
    "frame2_gray = cv2.cvtColor(frame2_original,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "frame1_gray_resized = cv2.resize(frame1_gray, (FIXED_WIDTH, FIXED_HEIGHT))\n",
    "frame2_gray_resized = cv2.resize(frame1_gray, (FIXED_WIDTH, FIXED_HEIGHT))\n",
    "\n",
    "show2images(frame1_gray_resized, frame2_gray_resized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Calculate the optical flow of the two standardized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flow = getOpticalFlow(frame1_gray_resized, frame2_gray_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the Histogram of Optical Flow (HoF) from the given optica flow\n",
    "\n",
    "from scipy import sqrt, pi, arctan2, cos, sin\n",
    "from scipy.ndimage import uniform_filter\n",
    "def hof(flow, orientations=9, pixels_per_cell=(10, 10),\n",
    "        cells_per_block=(4, 3), visualise=False, normalise=False, motion_threshold=1.):\n",
    "\n",
    "    \"\"\"Extract Histogram of Optical Flow (HOF) for a given image.\n",
    "    Key difference between this and HOG is that flow is MxNx2 instead of MxN\n",
    "    Compute a Histogram of Optical Flow (HOF) by\n",
    "        1. (optional) global image normalisation\n",
    "        2. computing the dense optical flow\n",
    "        3. computing flow histograms\n",
    "        4. normalising across blocks\n",
    "        5. flattening into a feature vector\n",
    "    Parameters\n",
    "    ----------\n",
    "    Flow : (M, N) ndarray\n",
    "        Input image (x and y flow images).\n",
    "    orientations : int\n",
    "        Number of orientation bins.\n",
    "    pixels_per_cell : 2 tuple (int, int)\n",
    "        Size (in pixels) of a cell.\n",
    "    cells_per_block  : 2 tuple (int,int)\n",
    "        Number of cells in each block.\n",
    "    visualise : bool, optional\n",
    "        Also return an image of the hof.\n",
    "    normalise : bool, optional\n",
    "        Apply power law compression to normalise the image before\n",
    "        processing.\n",
    "    static_threshold : threshold for no motion\n",
    "    Returns\n",
    "    -------\n",
    "    newarr : ndarray\n",
    "        hof for the image as a 1D (flattened) array.\n",
    "    hof_image : ndarray (if visualise=True)\n",
    "        A visualisation of the hof image.\n",
    "    References\n",
    "    ----------\n",
    "    * http://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\n",
    "    * Dalal, N and Triggs, B, Histograms of Oriented Gradients for\n",
    "      Human Detection, IEEE Computer Society Conference on Computer\n",
    "      Vision and Pattern Recognition 2005 San Diego, CA, USA\n",
    "    \"\"\"\n",
    "    flow = np.atleast_2d(flow)\n",
    "\n",
    "    \"\"\" \n",
    "    -1-\n",
    "    The first stage applies an optional global image normalisation\n",
    "    equalisation that is designed to reduce the influence of illumination\n",
    "    effects. In practice we use gamma (power law) compression, either\n",
    "    computing the square root or the log of each colour channel.\n",
    "    Image texture strength is typically proportional to the local surface\n",
    "    illumination so this compression helps to reduce the effects of local\n",
    "    shadowing and illumination variations.\n",
    "    \"\"\"\n",
    "\n",
    "    if flow.ndim < 3:\n",
    "        raise ValueError(\"Requires dense flow in both directions\")\n",
    "\n",
    "    if normalise:\n",
    "        flow = sqrt(flow)\n",
    "\n",
    "    \"\"\" \n",
    "    -2-\n",
    "    The second stage computes first order image gradients. These capture\n",
    "    contour, silhouette and some texture information, while providing\n",
    "    further resistance to illumination variations. The locally dominant\n",
    "    colour channel is used, which provides colour invariance to a large\n",
    "    extent. Variant methods may also include second order image derivatives,\n",
    "    which act as primitive bar detectors - a useful feature for capturing,\n",
    "    e.g. bar like structures in bicycles and limbs in humans.\n",
    "    \"\"\"\n",
    "\n",
    "    if flow.dtype.kind == 'u':\n",
    "        # convert uint image to float\n",
    "        # to avoid problems with subtracting unsigned numbers in np.diff()\n",
    "        flow = flow.astype('float')\n",
    "\n",
    "    gx = np.zeros(flow.shape[:2])\n",
    "    gy = np.zeros(flow.shape[:2])\n",
    "    # gx[:, :-1] = np.diff(flow[:,:,1], n=1, axis=1)\n",
    "    # gy[:-1, :] = np.diff(flow[:,:,0], n=1, axis=0)\n",
    "\n",
    "    gx = flow[:,:,1]\n",
    "    gy = flow[:,:,0]\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\" \n",
    "    -3-\n",
    "    The third stage aims to produce an encoding that is sensitive to\n",
    "    local image content while remaining resistant to small changes in\n",
    "    pose or appearance. The adopted method pools gradient orientation\n",
    "    information locally in the same way as the SIFT [Lowe 2004]\n",
    "    feature. The image window is divided into small spatial regions,\n",
    "    called \"cells\". For each cell we accumulate a local 1-D histogram\n",
    "    of gradient or edge orientations over all the pixels in the\n",
    "    cell. This combined cell-level 1-D histogram forms the basic\n",
    "    \"orientation histogram\" representation. Each orientation histogram\n",
    "    divides the gradient angle range into a fixed number of\n",
    "    predetermined bins. The gradient magnitudes of the pixels in the\n",
    "    cell are used to vote into the orientation histogram.\n",
    "    \"\"\"\n",
    "\n",
    "    magnitude = sqrt(gx**2 + gy**2)\n",
    "    orientation = arctan2(gy, gx) * (180 / pi) % 180\n",
    "\n",
    "    sy, sx = flow.shape[:2]\n",
    "    cx, cy = pixels_per_cell\n",
    "    bx, by = cells_per_block\n",
    "\n",
    "    n_cellsx = int(np.floor(sx // cx))  # number of cells in x\n",
    "    n_cellsy = int(np.floor(sy // cy))  # number of cells in y\n",
    "\n",
    "    # compute orientations integral images\n",
    "    orientation_histogram = np.zeros((n_cellsy, n_cellsx, orientations))\n",
    "    subsample = np.index_exp[cy / 2:cy * n_cellsy:cy, cx / 2:cx * n_cellsx:cx]\n",
    "    for i in range(orientations-1):\n",
    "        #create new integral image for this orientation\n",
    "        # isolate orientations in this range\n",
    "\n",
    "        temp_ori = np.where(orientation < 180 / orientations * (i + 1),\n",
    "                            orientation, -1)\n",
    "        temp_ori = np.where(orientation >= 180 / orientations * i,\n",
    "                            temp_ori, -1)\n",
    "        # select magnitudes for those orientations\n",
    "        cond2 = (temp_ori > -1) * (magnitude > motion_threshold)\n",
    "        temp_mag = np.where(cond2, magnitude, 0)\n",
    "\n",
    "        temp_filt = uniform_filter(temp_mag, size=(cy, cx))\n",
    "        orientation_histogram[:, :, i] = temp_filt[subsample]\n",
    "\n",
    "    ''' Calculate the no-motion bin '''\n",
    "    temp_mag = np.where(magnitude <= motion_threshold, magnitude, 0)\n",
    "\n",
    "    temp_filt = uniform_filter(temp_mag, size=(cy, cx))\n",
    "    orientation_histogram[:, :, -1] = temp_filt[subsample]\n",
    "\n",
    "    # now for each cell, compute the histogram\n",
    "    hof_image = None\n",
    "\n",
    "    if visualise:\n",
    "        from skimage import draw\n",
    "\n",
    "        radius = min(cx, cy) // 2 - 1\n",
    "        hof_image = np.zeros((sy, sx), dtype=float)\n",
    "        for x in range(n_cellsx):\n",
    "            for y in range(n_cellsy):\n",
    "                for o in range(orientations-1):\n",
    "                    centre = tuple([y * cy + cy // 2, x * cx + cx // 2])\n",
    "                    dx = int(radius * cos(float(o) / orientations * np.pi))\n",
    "                    dy = int(radius * sin(float(o) / orientations * np.pi))\n",
    "                    rr, cc = draw.bresenham(centre[0] - dy, centre[1] - dx,\n",
    "                                            centre[0] + dy, centre[1] + dx)\n",
    "                    hof_image[rr, cc] += orientation_histogram[y, x, o]\n",
    "\n",
    "    \"\"\"\n",
    "    The fourth stage computes normalisation, which takes local groups of\n",
    "    cells and contrast normalises their overall responses before passing\n",
    "    to next stage. Normalisation introduces better invariance to illumination,\n",
    "    shadowing, and edge contrast. It is performed by accumulating a measure\n",
    "    of local histogram \"energy\" over local groups of cells that we call\n",
    "    \"blocks\". The result is used to normalise each cell in the block.\n",
    "    Typically each individual cell is shared between several blocks, but\n",
    "    its normalisations are block dependent and thus different. The cell\n",
    "    thus appears several times in the final output vector with different\n",
    "    normalisations. This may seem redundant but it improves the performance.\n",
    "    We refer to the normalised block descriptors as Histogram of Oriented\n",
    "    Gradient (hog) descriptors.\n",
    "    \"\"\"\n",
    "\n",
    "    n_blocksx = (n_cellsx - bx) + 1\n",
    "    n_blocksy = (n_cellsy - by) + 1\n",
    "    normalised_blocks = np.zeros((n_blocksy, n_blocksx,\n",
    "                                  by, bx, orientations))\n",
    "\n",
    "    for x in range(n_blocksx):\n",
    "        for y in range(n_blocksy):\n",
    "            block = orientation_histogram[y:y+by, x:x+bx, :]\n",
    "            eps = 1e-5\n",
    "            normalised_blocks[y, x, :] = block / sqrt(block.sum()**2 + eps)\n",
    "\n",
    "    \"\"\"\n",
    "    The final step collects the hof descriptors from all blocks of a dense\n",
    "    overlapping grid of blocks covering the detection window into a combined\n",
    "    feature vector for use in the window classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    if visualise:\n",
    "        return normalised_blocks.ravel(), hof_image\n",
    "    else:\n",
    "        return normalised_blocks.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hof_result = hof(flow, pixels_per_cell=(20,20), cells_per_block=(5,5))\n",
    "print hof_result.shape\n",
    "print hof_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nc: 232\n",
      "offset: 208\n",
      "train files: 416\n",
      "train labels: 416\n",
      "test files: 48\n",
      "test labels: 48\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import os\n",
    "\n",
    "# Collect the file path of all the running and walking videos, we will only be using these 2 classes\n",
    "RUN_DIR = \"hof/run/\"\n",
    "RUN_FILES = os.listdir(RUN_DIR)\n",
    "RUN_FILES = [RUN_DIR + f for f in RUN_FILES]\n",
    "WALK_DIR = \"hof/walk/\"\n",
    "WALK_FILES = os.listdir(WALK_DIR)\n",
    "WALK_FILES = [WALK_DIR + f for f in WALK_FILES]\n",
    "\n",
    "# Use equal number of data from each class\n",
    "nc = min(len(RUN_FILES), len(WALK_FILES))\n",
    "print \"nc:\", nc\n",
    "RUN_FILES = RUN_FILES[0:nc]\n",
    "WALK_FILES = WALK_FILES[0:nc]\n",
    "\n",
    "RATIO = 0.9\n",
    "offset = int(np.floor(nc*RATIO))\n",
    "print \"offset:\", offset\n",
    "\n",
    "# Split test and training at a ratio of 1:9\n",
    "train_files = RUN_FILES[0:offset] + WALK_FILES[0:offset]\n",
    "test_files = RUN_FILES[offset:nc] + WALK_FILES[offset:nc]\n",
    "\n",
    "# Put the labels in vectors\n",
    "train_labels = np.zeros(offset*2, int)\n",
    "train_labels[0:offset] = 1 #RUN=1\n",
    "train_labels[offset:offset*2] = 2 #WALK=2\n",
    "\n",
    "test_len = nc-offset\n",
    "test_labels = np.zeros(test_len*2, int)\n",
    "test_labels[0:test_len] = 1 #RUN=1\n",
    "test_labels[test_len:test_len*2] = 2 #WALK=2\n",
    "\n",
    "print \"train files:\", len(train_files)\n",
    "print \"train labels:\", len(train_labels)\n",
    "print \"test files:\", len(test_files)\n",
    "print \"test labels:\", len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize the frames to gray-scale and equal dimensions\n",
    "\n",
    "FIXED_WIDTH = 160\n",
    "FIXED_HEIGHT = 120\n",
    "def normalizeFrame(frame_original):\n",
    "    frame_gray = cv2.cvtColor(frame_original,cv2.COLOR_BGR2GRAY)\n",
    "    frame_gray_resized = cv2.resize(frame_gray, (FIXED_WIDTH, FIXED_HEIGHT))\n",
    "    return frame_gray_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the Histogram of Optical Flow from two images\n",
    "def getHoF(frame1, frame2):\n",
    "    flow = getOpticalFlow(frame1, frame2)\n",
    "    return hof(flow, pixels_per_cell=(20,20), cells_per_block=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the Histogram of Optical Flows of a video grouped sequentially in a 1D array\n",
    "def getSequentialHoF(video_path):\n",
    "    hofs = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret1, frame1 = cap.read()\n",
    "    frame1 = normalizeFrame(frame1)\n",
    "    while(cap.isOpened()):\n",
    "        ret2, frame2 = cap.read()\n",
    "        if ret2 == True:\n",
    "            frame2 = normalizeFrame(frame2)\n",
    "            hof_array = getHoF(frame1, frame2)\n",
    "            hofs = np.concatenate((hofs, hof_array),axis=0)\n",
    "#             show2images(frame1, frame2)\n",
    "            frame1 = frame2\n",
    "        else:\n",
    "            break\n",
    "    return hofs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the training set and test set, which are arrays of sequential HoF for each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = [getSequentialHoF(p) for p in train_files]\n",
    "test = [getSequentialHoF(p) for p in test_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem is, the rows have varying lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131400\n",
      "86400\n"
     ]
    }
   ],
   "source": [
    "print len(test[1])\n",
    "print len(test[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the row with the largest length in training set and testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data scaling\n",
    "Lets attempt to scale each vector like an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_width = max(np.array([len(i) for i in train]).max(),np.array([len(i) for i in test]).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "\n",
    "# Find the max length of the vector\n",
    "def longest(a):\n",
    "    return max(len(a), *map(longest, a)) if isinstance(a, list) and a else 0\n",
    "\n",
    "longest_train = longest(train)\n",
    "longest_test = longest(test)\n",
    "\n",
    "def scale_vec(vec, longest): \n",
    "    scale_to = 1.0 * longest/len(vec)\n",
    "    resized = cv2.resize(vec, (max_width, 1))\n",
    "    return resized[0].T\n",
    "    # return #scipy.misc.imresize(vec, scale_to, interp='nearest', mode=\"L\")\n",
    "\n",
    "test_resized = [scale_vec(x, longest_test) for x in test]\n",
    "train_resized = [scale_vec(x, longest_train) for x in train]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method pads each row of the 2D array, with 0, to a specified width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def numpy_fillna(data, width):\n",
    "    # Get lengths of each row of data\n",
    "    lens = np.array([len(i) for i in data])\n",
    "\n",
    "    # Mask of valid places in each row\n",
    "    mask = np.arange(width) < lens[:,None]\n",
    "\n",
    "    # Setup output array and put elements from data into masked positions\n",
    "    out = np.zeros(mask.shape, dtype=data.dtype)\n",
    "    out[mask] = np.concatenate(data)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are changing every row length to the maximum row length in testing and training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_pad = numpy_fillna(np.array(train), max_width)\n",
    "test_pad = numpy_fillna(np.array(test), max_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(train_pad, train_labels)\n",
    "predict = clf.predict(test_pad)\n",
    "\n",
    "\n",
    "clf.fit(train_resized, train_labels)\n",
    "predict_resized = clf.predict(test_resized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the accuracy of the SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded 0.625\n",
      "Resized 0.5625\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print \"Padded\", metrics.accuracy_score(predict, test_labels)\n",
    "print \"Resized\", metrics.accuracy_score(predict_resized, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded 0.604166666667\n",
      "Resized 0.541666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "tree_clf = tree.DecisionTreeClassifier()\n",
    "tree_clf.fit(train_pad, train_labels)\n",
    "predict = tree_clf.predict(test_resized)\n",
    "\n",
    "tree_clf.fit(train_resized, train_labels)\n",
    "predict_resized = tree_clf.predict(test_resized)\n",
    "\n",
    "\n",
    "print \"Padded\", metrics.accuracy_score(predict, test_labels)\n",
    "print \"Resized\", metrics.accuracy_score(predict_resized, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded 0.604166666667\n",
      "Resized 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "rf_clf = ensemble.RandomForestClassifier()\n",
    "rf_clf.fit(train_pad, train_labels)\n",
    "predict = rf_clf.predict(test_pad)\n",
    "\n",
    "rf_clf.fit(train_resized, train_labels)\n",
    "predict_resized = rf_clf.predict(test_resized)\n",
    "\n",
    "\n",
    "print \"Padded\", metrics.accuracy_score(predict, test_labels)\n",
    "print \"Resized\", metrics.accuracy_score(predict_resized, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded 0.583333333333\n",
      "Resized 0.604166666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "lr_clf = linear_model.LogisticRegression()\n",
    "lr_clf.fit(train_pad, train_labels)\n",
    "predict = lr_clf.predict(test_pad)\n",
    "\n",
    "lr_clf.fit(train_resized, train_labels)\n",
    "predict_resized = lr_clf.predict(test_resized)\n",
    "\n",
    "\n",
    "print \"Padded\", metrics.accuracy_score(predict, test_labels)\n",
    "print \"Resized\", metrics.accuracy_score(predict_resized, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KNN DTW GITHUB METHODS\n",
    "\n",
    "class KnnDtw(object):\n",
    "    \"\"\"K-nearest neighbor classifier using dynamic time warping\n",
    "    as the distance measure between pairs of time series arrays\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    n_neighbors : int, optional (default = 5)\n",
    "        Number of neighbors to use by default for KNN\n",
    "        \n",
    "    max_warping_window : int, optional (default = infinity)\n",
    "        Maximum warping window allowed by the DTW dynamic\n",
    "        programming function\n",
    "            \n",
    "    subsample_step : int, optional (default = 1)\n",
    "        Step size for the timeseries array. By setting subsample_step = 2,\n",
    "        the timeseries length will be reduced by 50% because every second\n",
    "        item is skipped. Implemented by x[:, ::subsample_step]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_neighbors=5, max_warping_window=10000, subsample_step=1):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.max_warping_window = max_warping_window\n",
    "        self.subsample_step = subsample_step\n",
    "    \n",
    "    def fit(self, x, l):\n",
    "        \"\"\"Fit the model using x as training data and l as class labels\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        x : array of shape [n_samples, n_timepoints]\n",
    "            Training data set for input into KNN classifer\n",
    "            \n",
    "        l : array of shape [n_samples]\n",
    "            Training labels for input into KNN classifier\n",
    "        \"\"\"\n",
    "        \n",
    "        self.x = x\n",
    "        self.l = l\n",
    "        \n",
    "    def _dtw_distance(self, ts_a, ts_b, d = lambda x,y: abs(x-y)):\n",
    "        \"\"\"Returns the DTW similarity distance between two 2-D\n",
    "        timeseries numpy arrays.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        ts_a, ts_b : array of shape [n_samples, n_timepoints]\n",
    "            Two arrays containing n_samples of timeseries data\n",
    "            whose DTW distance between each sample of A and B\n",
    "            will be compared\n",
    "        \n",
    "        d : DistanceMetric object (default = abs(x-y))\n",
    "            the distance measure used for A_i - B_j in the\n",
    "            DTW dynamic programming function\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        DTW distance between A and B\n",
    "        \"\"\"\n",
    "\n",
    "        # Create cost matrix via broadcasting with large int\n",
    "        ts_a, ts_b = np.array(ts_a), np.array(ts_b)\n",
    "        M, N = len(ts_a), len(ts_b)\n",
    "        cost = sys.maxint * np.ones((M, N))\n",
    "\n",
    "        # Initialize the first row and column\n",
    "        cost[0, 0] = d(ts_a[0], ts_b[0])\n",
    "        for i in xrange(1, M):\n",
    "            cost[i, 0] = cost[i-1, 0] + d(ts_a[i], ts_b[0])\n",
    "\n",
    "        for j in xrange(1, N):\n",
    "            cost[0, j] = cost[0, j-1] + d(ts_a[0], ts_b[j])\n",
    "\n",
    "        # Populate rest of cost matrix within window\n",
    "        for i in xrange(1, M):\n",
    "            for j in xrange(max(1, i - self.max_warping_window),\n",
    "                            min(N, i + self.max_warping_window)):\n",
    "                choices = cost[i - 1, j - 1], cost[i, j-1], cost[i-1, j]\n",
    "                cost[i, j] = min(choices) + d(ts_a[i], ts_b[j])\n",
    "\n",
    "        # Return DTW distance given window \n",
    "        return cost[-1, -1]\n",
    "    \n",
    "    def _dist_matrix(self, x, y):\n",
    "        \"\"\"Computes the M x N distance matrix between the training\n",
    "        dataset and testing dataset (y) using the DTW distance measure\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        x : array of shape [n_samples, n_timepoints]\n",
    "        \n",
    "        y : array of shape [n_samples, n_timepoints]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Distance matrix between each item of x and y with\n",
    "            shape [training_n_samples, testing_n_samples]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute the distance matrix        \n",
    "        dm_count = 0\n",
    "        \n",
    "        # Compute condensed distance matrix (upper triangle) of pairwise dtw distances\n",
    "        # when x and y are the same array\n",
    "        if(np.array_equal(x, y)):\n",
    "            x_s = shape(x)\n",
    "            dm = np.zeros((x_s[0] * (x_s[0] - 1)) // 2, dtype=np.double)\n",
    "            \n",
    "          \n",
    "            \n",
    "            for i in xrange(0, x_s[0] - 1):\n",
    "                for j in xrange(i + 1, x_s[0]):\n",
    "                    dm[dm_count] = self._dtw_distance(x[i, ::self.subsample_step],\n",
    "                                                      y[j, ::self.subsample_step])\n",
    "                    \n",
    "                    dm_count += 1\n",
    "            \n",
    "            # Convert to squareform\n",
    "            dm = squareform(dm)\n",
    "            return dm\n",
    "        \n",
    "        # Compute full distance matrix of dtw distnces between x and y\n",
    "        else:\n",
    "            x_s = np.shape(x)\n",
    "            y_s = np.shape(y)\n",
    "            dm = np.zeros((x_s[0], y_s[0])) \n",
    "            dm_size = x_s[0]*y_s[0]\n",
    "            \n",
    "           \n",
    "        \n",
    "            for i in xrange(0, x_s[0]):\n",
    "                for j in xrange(0, y_s[0]):\n",
    "                    dm[i, j] = self._dtw_distance(x[i, ::self.subsample_step],\n",
    "                                                  y[j, ::self.subsample_step])\n",
    "                    # Update progress bar\n",
    "                    dm_count += 1\n",
    "        \n",
    "            return dm\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict the class labels or probability estimates for \n",
    "        the provided data\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "          x : array of shape [n_samples, n_timepoints]\n",
    "              Array containing the testing data set to be classified\n",
    "          \n",
    "        Returns\n",
    "        -------\n",
    "          2 arrays representing:\n",
    "              (1) the predicted class labels \n",
    "              (2) the knn label count probability\n",
    "        \"\"\"\n",
    "        \n",
    "        dm = self._dist_matrix(x, self.x)\n",
    "\n",
    "        # Identify the k nearest neighbors\n",
    "        knn_idx = dm.argsort()[:, :self.n_neighbors]\n",
    "\n",
    "        # Identify k nearest labels\n",
    "        knn_labels = self.l[knn_idx]\n",
    "        \n",
    "        # Model Label\n",
    "        mode_data = mode(knn_labels, axis=1)\n",
    "        mode_label = mode_data[0]\n",
    "        mode_proba = mode_data[1]/self.n_neighbors\n",
    "\n",
    "        return mode_label.ravel(), mode_proba.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_crossval = cross_val_score(rf_clf, train_pad, \n",
    "                train_labels, \n",
    "                verbose=1, \n",
    "                n_jobs=-1, \n",
    "                cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_crossval = cross_val_score(clf, train_pad, \n",
    "                train_labels, \n",
    "                verbose=1, \n",
    "                n_jobs=-1, \n",
    "                cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
